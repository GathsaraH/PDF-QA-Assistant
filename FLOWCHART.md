# ğŸ”„ Complete RAG System Flowchart

## ğŸ“Š System Architecture Flow

```mermaid
graph TB
    User[ğŸ‘¤ User] -->|1. Upload PDF| Frontend[ğŸŒ Frontend<br/>Next.js :3000]
    
    Frontend -->|2. POST /api/upload<br/>multipart/form-data| Backend[âš™ï¸ Backend<br/>FastAPI :8000]
    
    Backend -->|3. Save File| FileSystem[ğŸ’¾ File System<br/>uploads/]
    
    Backend -->|4. Extract Text| PDFProcessor[ğŸ“„ PDF Processor<br/>pdfplumber]
    
    PDFProcessor -->|5. Raw Text| Chunker[âœ‚ï¸ Text Chunker<br/>Split into chunks]
    
    Chunker -->|6. Text Chunks<br/>Array of strings| Embedder[ğŸ”¢ Embedding Generator<br/>Gemini text-embedding-004]
    
    Embedder -->|7. Vector Embeddings<br/>768 dimensions| Pinecone[ğŸ—„ï¸ Pinecone<br/>Vector Database<br/>Namespace: session_id]
    
    Backend -->|8. Save Metadata| Database[(ğŸ’¾ PostgreSQL<br/>Document metadata)]
    
    User -->|9. Ask Question| Frontend
    
    Frontend -->|10. POST /api/chat<br/>JSON: question, session_id| Backend
    
    Backend -->|11. Get Last Message| Database
    
    Database -->|12. Last 1 Message<br/>for context| Backend
    
    Backend -->|13. Create Question Embedding| Embedder
    
    Embedder -->|14. Query Vector| Pinecone
    
    Pinecone -->|15. Top 3 Similar Chunks<br/>with metadata| Backend
    
    Backend -->|16. Build Context<br/>Question + Last Message + Chunks| LLM[ğŸ¤– Gemini LLM<br/>gemini-flash-latest]
    
    LLM -->|17. Generated Answer<br/>with sources| Backend
    
    Backend -->|18. Save Messages| Database
    
    Backend -->|19. Track Token Usage| Database
    
    Backend -->|20. JSON Response<br/>answer, sources, tokens| Frontend
    
    Frontend -->|21. Display Answer<br/>with citations| User
```

---

## ğŸ” Detailed Step-by-Step Flow

### **Phase 1: Document Upload & Processing**

1. **User Uploads PDF** â†’ Frontend receives file
2. **Frontend â†’ Backend** â†’ `POST /api/upload` with file + session_id
3. **Backend Saves File** â†’ Stores in `uploads/` directory
4. **Extract Text** â†’ `pdfplumber` extracts all text from PDF
5. **Chunk Text** â†’ Split into chunks (500 chars, 50 overlap)
6. **Create Embeddings** â†’ For EACH chunk:
   - Call Gemini `text-embedding-004`
   - Get 768-dimensional vector
7. **Store in Pinecone** â†’ Each chunk stored with:
   - Vector (768 dims)
   - Metadata: `{text, chunk_index, session_id}`
   - Namespace: `session_id`
8. **Save to Database** â†’ Document metadata:
   - session_id, filename, file_size, chunk_count

### **Phase 2: Question Answering**

9. **User Asks Question** â†’ Frontend sends question
10. **Frontend â†’ Backend** â†’ `POST /api/chat` with question + session_id
11. **Get Context** â†’ Backend queries database for:
    - Last 1 message (for conversation context)
12. **Create Question Embedding** â†’ Embed the question using Gemini
13. **Query Pinecone** â†’ Search for top 3 similar chunks:
    - Uses cosine similarity
    - Filters by session_id namespace
14. **Retrieve Chunks** â†’ Get the actual text chunks from results
15. **Build Prompt** â†’ Combine:
    - Last message (if exists)
    - Current question
    - Retrieved chunks (as context)
16. **Call LLM** â†’ Send to Gemini `gemini-flash-latest`:
    - Falls back to `gemini-pro-latest` if quota exceeded
17. **Get Answer** â†’ LLM returns answer with source references
18. **Save to Database** â†’ Store both:
    - User message
    - Assistant message (with sources)
    - Token usage tracking
19. **Return Response** â†’ JSON with:
    - `answer`: The generated answer
    - `sources`: Array of source chunk texts
    - `token_count`: Total tokens used
20. **Frontend Displays** â†’ Shows answer with clickable citations

---

## ğŸ¯ Key Features

### **Token Optimization (POC)**
- âœ… Only last 1 message used for context
- âœ… Prevents token overflow on free tier
- âœ… Sliding window approach

### **Vector Search**
- âœ… Semantic similarity search
- âœ… Top 3 most relevant chunks
- âœ… Session-based isolation (namespace)

### **Database Integration**
- âœ… Document metadata storage
- âœ… Chat history persistence
- âœ… Token usage tracking

---

## ğŸ”§ Technology Stack

| Component | Technology |
|-----------|-----------|
| Frontend | Next.js 14, React, TypeScript |
| Backend | FastAPI, Python 3.13 |
| PDF Processing | pdfplumber |
| Embeddings | Google Gemini `text-embedding-004` (768 dims) |
| Vector DB | Pinecone (Serverless) |
| LLM | Google Gemini `gemini-flash-latest` |
| Database | PostgreSQL (Supabase/Neon) |
| ORM | SQLAlchemy |

---

## ğŸ“ Important Notes

1. **Embeddings are created AFTER chunking** - Each chunk gets its own embedding
2. **Pinecone stores vectors + metadata** - Not just vectors
3. **Session isolation** - Each document has its own Pinecone namespace
4. **Token optimization** - Only last 1 message for context (POC requirement)
5. **Fallback mechanism** - Gemini Flash â†’ Gemini Pro if quota exceeded

---

## ğŸš€ Flow Summary

```
Upload: PDF â†’ Text â†’ Chunks â†’ Embeddings â†’ Pinecone + Database
Query: Question â†’ Embedding â†’ Pinecone Search â†’ Chunks + Last Message â†’ LLM â†’ Answer â†’ Database â†’ Frontend
```

---

**This flowchart is now accurate!** âœ…

